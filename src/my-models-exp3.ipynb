{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from base.data import get_data_loaders, get_data_loaders_wandb, get_data_loaders_loocv\n",
    "from base.train import Trainer, eval_metrics\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ec987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_norm, dropout, n_layers):\n",
    "        super(MLPHead, self).__init__()\n",
    "        l_sz = [in_dim] + [in_dim//2**(i+1) for i in range(n_layers)] + [out_dim]\n",
    "        layers = []\n",
    "        for i in range(1, len(l_sz)-1):\n",
    "            layers.append(nn.Linear(l_sz[i-1], l_sz[i]))\n",
    "            if use_norm == 'batch':\n",
    "                layers.append(nn.BatchNorm1d(l_sz[i]))\n",
    "            elif use_norm == 'layer':\n",
    "                layers.append(nn.LayerNorm(l_sz[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(l_sz[-2], l_sz[-1]))\n",
    "        self.head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.n_embeddings = cfg['n_embeddings']\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], cfg['emb_latent_dim']),\n",
    "            nn.LayerNorm(cfg['emb_latent_dim'])\n",
    "        )\n",
    "        # in_dim = cfg['emb_latent_dim'] * self.n_embeddings + cfg['transf_dim']\n",
    "        in_dim = cfg['emb_latent_dim'] * (self.n_embeddings[0] + self.n_embeddings[1]) + cfg['transf_dim']\n",
    "        out_dim = 1 if cfg['task'] == 'regression' else cfg['num_classes']\n",
    "        self.head = MLPHead(in_dim, out_dim, cfg['use_layer_norm'], cfg['dropout'], cfg['n_hid_layers'])\n",
    "\n",
    "    # ENB1: unmodified code\n",
    "    # EMB2: mutated code\n",
    "    def forward(self, batch):\n",
    "        if self.n_embeddings[0]:\n",
    "            em_feat_1 = self.projection(batch[0])\n",
    "            if self.n_embeddings[1] == True:\n",
    "                em_feat_2 = self.projection(batch[1])\n",
    "                if self.n_embeddings[2]:\n",
    "                    x = torch.cat([em_feat_1, em_feat_2, batch[2]], dim=1)\n",
    "                else:\n",
    "                    x = torch.cat([em_feat_1, em_feat_2], dim=1)\n",
    "            else:\n",
    "                if self.n_embeddings[2]:\n",
    "                    x = torch.cat([em_feat_1, batch[1]], dim=1)\n",
    "                else:\n",
    "                    x = em_feat_1\n",
    "        else:\n",
    "            if self.n_embeddings[1]:\n",
    "                em_feat_2 = self.projection(batch[0])\n",
    "                if self.n_embeddings[2]:\n",
    "                    x = torch.cat([em_feat_2, batch[1]], dim=1)\n",
    "                else:\n",
    "                    x = em_feat_2\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "        # Old version:\n",
    "        # (EMB1, TRANSF) (case 1)\n",
    "        # (EMB1, EMB2, TRANSF) (case 2)\n",
    "        \n",
    "        # em_feat_1 = self.projection(batch[0])\n",
    "        # if self.n_embeddings == 1:\n",
    "        #     x = torch.cat([em_feat_1, batch[1]], dim=1)\n",
    "        # elif self.n_embeddings == 2:\n",
    "        #     em_feat_2 = self.projection(batch[1])\n",
    "        #     x = torch.cat([em_feat_1, em_feat_2, batch[2]], dim=1)\n",
    "        # return self.head(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600b6ce",
   "metadata": {},
   "source": [
    "## Main Project Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, cfg_data, device):\n",
    "        self.cfg_data = cfg_data\n",
    "        self.cfg_model = None\n",
    "        self.device = device\n",
    "        self._setup_data_loaders()\n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _setup_data_loaders(self):\n",
    "        self.tr_loader, self.va_loader = get_data_loaders_wandb(self.cfg_data)\n",
    "        \n",
    "    def _initialize_model(self):\n",
    "        self.cfg_model = {\n",
    "            'n_hid_layers': 3,\n",
    "            'n_embeddings': self.cfg_data['n_embeddings'],\n",
    "            'emb_dim': self.tr_loader.dataset.embed_dim, # embedding dimensionality\n",
    "            'emb_latent_dim': 256, # embedding projection dimensionality\n",
    "            'transf_dim': self.tr_loader.dataset.tr_dim, # transformation dimensionality\n",
    "            'model_type': 'mlp', # 'attention', 'mlp'\n",
    "            'task': self.cfg_data['task'],\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 1e-3,\n",
    "            'use_layer_norm': 'batch',\n",
    "            'weight_decay': 1e-3,\n",
    "            'max_epochs': 100,\n",
    "            'num_classes': len(self.cfg_data['classes'])\n",
    "        }\n",
    "        \n",
    "        if self.cfg_model['model_type'] == 'attention':\n",
    "            self.model = BidirectionalCrossAttention(self.cfg_model).to(self.device)\n",
    "        elif self.cfg_model['model_type'] == 'mlp':\n",
    "            self.model = MLP(self.cfg_model).to(self.device)\n",
    "        print(\"\\n\", self.model)\n",
    "    \n",
    "    def start_run(self, project_name, group_name, run_name, reinit=False):\n",
    "        wandb.init(\n",
    "            project=project_name, \n",
    "            group=group_name, \n",
    "            name=run_name, \n",
    "            reinit=reinit, \n",
    "            config={**self.cfg_data, **self.cfg_model}\n",
    "        )\n",
    "        wandb.config.update(self.cfg_data)\n",
    "        wandb.config.update(self.cfg_model)\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.trainer = Trainer(self.device, self.model, self.cfg_model)\n",
    "        tr_losses, va_losses = self.trainer.train(self.tr_loader, self.va_loader)\n",
    "        self.model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "        wandb.log({\"training_loss\": tr_losses[-1], \"validation_loss\": va_losses[-1]})\n",
    "        return tr_losses, va_losses\n",
    "\n",
    "    def evaluate_model(self, loader, dataset_type):\n",
    "        preds, tgts = self.trainer.predict(loader)\n",
    "        mapping = loader.dataset.loop_mapping if self.cfg_data['min_transformations'] > 2 else None\n",
    "        unique_classes = tgts.unique().tolist()\n",
    "        active_classes = [self.cfg_data['classes'][i] for i in unique_classes]\n",
    "        metrics = eval_metrics(\n",
    "            preds, tgts, figures=True, mapping=mapping, type=self.cfg_data['task'],\n",
    "            classes=active_classes\n",
    "        )\n",
    "        print(f\"{dataset_type} metrics: {metrics}\")\n",
    "        wandb.log({f\"{dataset_type}_metrics\": metrics})\n",
    "        \n",
    "    def sample_data_hyperparameters(self, method='random'):\n",
    "        return {\n",
    "            'batch_size': random.choice([128, 256, 512, 1024, 2048, 2196, 3680, 4096]),\n",
    "        }\n",
    "        \n",
    "    def sample_model_hyperparameters(self, method='random'):\n",
    "        return {\n",
    "            'learning_rate': random.uniform(1e-5, 1e-2),\n",
    "            'dropout': random.uniform(0.1, 0.5),\n",
    "            'n_hid_layers': random.randint(1, 5),\n",
    "            'emb_latent_dim': random.choice([128, 256, 512]),\n",
    "            'weight_decay': random.uniform(1e-5, 1e-3),\n",
    "            'use_layer_norm': random.choice(['batch', 'layer', None])\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, project_name, group_name, run_prefix, n_runs, n_start=1):\n",
    "        for r in range(n_start, n_runs+1):\n",
    "            print(f\"Run {r}\")\n",
    "            \n",
    "            # Sample hyperparameters\n",
    "            self.cfg_model.update(self.sample_model_hyperparameters())\n",
    "            self.cfg_data.update(self.sample_data_hyperparameters())\n",
    "            \n",
    "            self.start_run(project_name, group_name, f\"{run_prefix}_{r}\", reinit=True)\n",
    "            \n",
    "            self.train_model()\n",
    "            self.evaluate_model(self.tr_loader, 'train')\n",
    "            self.evaluate_model(self.va_loader, 'val')\n",
    "            \n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815996db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "        \n",
    "cfg_data = {\n",
    "    'n_workers': 32,\n",
    "    'batch_size': 2048,    \n",
    "    'stratification': 'majority', # 'random', 'binary', 'clustered', 'majority'\n",
    "    'data_path': '/mnt/fastdata/datasets/lore-loops/web-clang',\n",
    "    'embedding_model': 'source_coderankembed', # 'llvm_llmcompiler', 'source_codellama', 'source_llmcompiler', 'source_codet5p', 'source_coderankembed'\n",
    "    'embedding_layer': 'last', # 'last', 'all' (not working yet)\n",
    "    'n_embeddings': [1, 0, 1], # binary flags for [ref_embedding, transformation_embedding, transformation_encoding]\n",
    "    'min_transformations': 1, # only loop groups with at least <min> transformations\n",
    "    'max_transformations': 10000, # only loop groups with at most <max> transformations\n",
    "    'max_speedup': 2.5, # remove any loop group with a speedup transformation > max_speedup\n",
    "    'max_source_size': 100000, # only loop groups where reference .C file is at most <max> bytes\n",
    "    'task': 'classification',  # 'classification', 'regression'\n",
    "    # [(0, np.float64(0.5)), (2, np.float64(1.5)), (4, np.float64(2.5)), (6, np.float64(3.5))]\n",
    "    'class_splits': [0, .7, 1, 1.5, 17],\n",
    "    # 'class_splits': [-100, .5, 0.9999, 1.0001, 1.5, 2.5, 3.5, 100000],\n",
    "    'classes': ['HSL', 'SL', 'SP', 'HSP'],\n",
    "    'seed': 4,\n",
    "    'csv_path': './clang_4.0.0_data_table.csv', # for new data loader\n",
    "    'benchmark': 'NPB',\n",
    "    'filters': [('benchmark', '==', 'NPB'), ('application', '==', 'SP')],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b92e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_runner = ExperimentRunner(cfg_data, device)\n",
    "experiment_runner.run_experiment(\n",
    "    project_name='MLFinal',\n",
    "    group_name='NPB Benchmark, SP App, Experiment 3 (Param Random Search)',\n",
    "    run_prefix=\"exp3_run\",\n",
    "    n_runs=40,\n",
    "    n_start=39\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee8fd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5384013",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# REQUIRES A SECOND LOOK\n",
    "##########################################\n",
    "class BidirectionalCrossAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.n_embeddings = cfg['n_embeddings']\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], cfg['emb_latent_dim']),\n",
    "            nn.LayerNorm(cfg['emb_latent_dim'])\n",
    "        )\n",
    "        latent_dim = cfg['emb_latent_dim'] * self.n_embeddings\n",
    "        out_dim = 1 if cfg['task'] == 'regression' else cfg['num_classes']\n",
    "        self.head = MLPHead(latent_dim+cfg['transf_dim'], out_dim, cfg['use_layer_norm'], cfg['dropout'], cfg['n_hid_layers'])\n",
    "        self.cross_attention1 = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=4, dropout=cfg['dropout'], batch_first=True, kdim=cfg['transf_dim'], vdim=cfg['transf_dim'])\n",
    "        self.cross_attention2 = nn.MultiheadAttention(embed_dim=cfg['transf_dim'], num_heads=4, dropout=cfg['dropout'], batch_first=True, kdim=latent_dim, vdim=latent_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        emb_proj_1 = self.projection(batch[0])\n",
    "        if self.n_embeddings == 1:\n",
    "            features1, _ = self.cross_attention1(query=emb_proj_1, key=batch[1], value=batch[1])\n",
    "            features2, _ = self.cross_attention2(query=batch[1], key=emb_proj_1, value=emb_proj_1)\n",
    "            return self.head(torch.cat([emb_proj_1+features1, batch[1]+features2], dim=1)).squeeze(-1)\n",
    "        elif self.n_embeddings == 2:\n",
    "            emb_proj_2 = self.projection(batch[1])\n",
    "            embs = torch.cat([emb_proj_1, emb_proj_2], dim=1)\n",
    "            features1, _ = self.cross_attention1(query=embs, key=batch[2], value=batch[2])\n",
    "            features2, _ = self.cross_attention2(query=batch[2], key=embs, value=embs)\n",
    "            return self.head(torch.cat([embs+features1, batch[2]+features2], dim=1)).squeeze(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
