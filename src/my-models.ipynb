{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f4c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from base.data import get_data_loaders, get_data_loaders_wandb, get_data_loaders_loocv\n",
    "from base.train import Trainer, eval_metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56ec987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, use_norm, dropout, n_layers):\n",
    "        super(MLPHead, self).__init__()\n",
    "        l_sz = [in_dim] + [in_dim//2**(i+1) for i in range(n_layers)] + [out_dim]\n",
    "        layers = []\n",
    "        for i in range(1, len(l_sz)-1):\n",
    "            layers.append(nn.Linear(l_sz[i-1], l_sz[i]))\n",
    "            if use_norm == 'batch':\n",
    "                layers.append(nn.BatchNorm1d(l_sz[i]))\n",
    "            elif use_norm == 'layer':\n",
    "                layers.append(nn.LayerNorm(l_sz[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(l_sz[-2], l_sz[-1]))\n",
    "        self.head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d16c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.n_embeddings = cfg['n_embeddings']\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], cfg['emb_latent_dim']),\n",
    "            nn.LayerNorm(cfg['emb_latent_dim'])\n",
    "        )\n",
    "        # in_dim = cfg['emb_latent_dim'] * self.n_embeddings + cfg['transf_dim']\n",
    "        in_dim = cfg['emb_latent_dim'] * (self.n_embeddings[0] + self.n_embeddings[1]) + cfg['transf_dim']\n",
    "        out_dim = 1 if cfg['task'] == 'regression' else cfg['num_classes']\n",
    "        self.head = MLPHead(in_dim, out_dim, cfg['use_layer_norm'], cfg['dropout'], cfg['n_hid_layers'])\n",
    "\n",
    "    # ENB1: unmodified code\n",
    "    # EMB2: mutated code\n",
    "    def forward(self, batch):\n",
    "        if self.n_embeddings[0]:\n",
    "            em_feat_1 = self.projection(batch[0])\n",
    "            if self.n_embeddings[1] == True:\n",
    "                em_feat_2 = self.projection(batch[1])\n",
    "                if self.n_embeddings[2]:\n",
    "                    x = torch.cat([em_feat_1, em_feat_2, batch[2]], dim=1)\n",
    "                else:\n",
    "                    x = torch.cat([em_feat_1, em_feat_2], dim=1)\n",
    "            else:\n",
    "                if self.n_embeddings[2]:\n",
    "                    x = torch.cat([em_feat_1, batch[1]], dim=1)\n",
    "                else:\n",
    "                    x = em_feat_1\n",
    "        else:\n",
    "            if self.n_embeddings[1]:\n",
    "                em_feat_2 = self.projection(batch[0])\n",
    "                if self.n_embeddings[2]:\n",
    "                    x = torch.cat([em_feat_2, batch[1]], dim=1)\n",
    "                else:\n",
    "                    x = em_feat_2\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "        # Old version:\n",
    "        # (EMB1, TRANSF) (case 1)\n",
    "        # (EMB1, EMB2, TRANSF) (case 2)\n",
    "        \n",
    "        # em_feat_1 = self.projection(batch[0])\n",
    "        # if self.n_embeddings == 1:\n",
    "        #     x = torch.cat([em_feat_1, batch[1]], dim=1)\n",
    "        # elif self.n_embeddings == 2:\n",
    "        #     em_feat_2 = self.projection(batch[1])\n",
    "        #     x = torch.cat([em_feat_1, em_feat_2, batch[2]], dim=1)\n",
    "        # return self.head(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4939713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loocv_splits(loaders):\n",
    "    def plot(train_sizes, val_sizes, labels, ylabel, title):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = range(len(train_sizes))\n",
    "        ax.bar(x, train_sizes, width=0.4, label='Train')\n",
    "        ax.bar([i + 0.4 for i in x], val_sizes, width=0.4, label='Validation')\n",
    "        ax.set_xticks([i + 0.2 for i in x])\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        if ylabel == 'Data points':\n",
    "            ax.set_yscale('log')\n",
    "        # Add numbers to the bars\n",
    "        for i, v in enumerate(train_sizes):\n",
    "            ax.text(i, v + 10, str(v), ha='center')\n",
    "        for i, v in enumerate(val_sizes):\n",
    "            ax.text(i + 0.4, v + 10, str(v), ha='center')\n",
    "        plt.show()\n",
    "\n",
    "    # Get application names\n",
    "    app_names = [val_app for val_app, _, _ in loaders]\n",
    "\n",
    "    # Plot how many data points are in each fold of the LOOCV\n",
    "    train_sizes = [len(tr_loader.dataset) for _, tr_loader, _ in loaders]\n",
    "    val_sizes = [len(va_loader.dataset) for _, _, va_loader in loaders]\n",
    "    plot(train_sizes, val_sizes, app_names, 'Data points', 'Train and validation data points for each LOOCV split')\n",
    "\n",
    "    # Plot how many loop groups are in each fold of the LOOCV\n",
    "    train_sizes = [len(tr_loader.dataset.loop_ids) for _, tr_loader, _ in loaders]\n",
    "    val_sizes = [len(va_loader.dataset.loop_ids) for _, _, va_loader in loaders]\n",
    "    plot(train_sizes, val_sizes, app_names, 'Loop groups', 'Train and validation loop groups for each LOOCV split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600b6ce",
   "metadata": {},
   "source": [
    "## Main Project Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe785ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    def __init__(self, cfg_data, device, load_data=True):\n",
    "        self.cfg_data = cfg_data\n",
    "        self.device = device\n",
    "                \n",
    "    def _initialize_model(self, cfg_model):\n",
    "        if cfg_model['model_type'] == 'attention':\n",
    "            self.model = BidirectionalCrossAttention(cfg_model).to(self.device)\n",
    "        elif cfg_model['model_type'] == 'mlp':\n",
    "            self.model = MLP(cfg_model).to(self.device)\n",
    "        print(\"\\n\", self.model)\n",
    "\n",
    "    def train_model(self, cfg_model):\n",
    "        self.trainer = Trainer(self.device, self.model, cfg_model)\n",
    "        tr_losses, va_losses = self.trainer.train(self.tr_loader, self.va_loader)\n",
    "        self.model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "        wandb.log({\"training_loss\": tr_losses[-1], \"validation_loss\": va_losses[-1]})\n",
    "        return tr_losses, va_losses\n",
    "\n",
    "    def evaluate_model(self, loader, dataset_type):\n",
    "        preds, tgts = self.trainer.predict(loader)\n",
    "        mapping = loader.dataset.loop_mapping if self.cfg_data['min_transformations'] > 2 else None\n",
    "        unique_classes = tgts.unique().tolist()\n",
    "        active_classes = [self.cfg_data['classes'][i] for i in unique_classes]\n",
    "        metrics = eval_metrics(\n",
    "            preds, tgts, figures=True, mapping=mapping, type=self.cfg_data['task'],\n",
    "            classes=active_classes\n",
    "        )\n",
    "        print(f\"{dataset_type} metrics: {metrics}\")\n",
    "        wandb.log({f\"{dataset_type}_metrics\": metrics})\n",
    "\n",
    "    def run(self, project_name, group_name, run_name_prefix, count=1):\n",
    "        for i in range(count):\n",
    "            wandb.init(\n",
    "                project=project_name, \n",
    "                group=group_name,\n",
    "                name=f\"{run_name_prefix}_{i+1}\",\n",
    "                config={**self.cfg_data},\n",
    "            )\n",
    "            self.tr_loader, self.va_loader = get_data_loaders_wandb(self.cfg_data)\n",
    "            self.run_experiment()\n",
    "    \n",
    "    def run_loocv(self, project_name, group_name, run_name_prefix):\n",
    "        loaders = get_data_loaders_loocv(self.cfg_data)\n",
    "        # visualize_loocv_splits(loaders) # create a visualization of the train and validation sizes per fold\n",
    "        for i, (val_app, tr_loader, va_loader) in enumerate(loaders):\n",
    "            print(f'Fold {i+1} - Validation on {val_app}')\n",
    "            # update the loaders for the current fold\n",
    "            self.tr_loader = tr_loader\n",
    "            self.va_loader = va_loader\n",
    "            print(f'Training on {len(self.tr_loader.dataset)} samples')\n",
    "            print(f'Validating on {len(self.va_loader.dataset)} samples')\n",
    "            wandb.init(\n",
    "                project=project_name, \n",
    "                group=group_name,\n",
    "                name=f\"{run_name_prefix}_fold_{i+1}_{val_app}\",\n",
    "                config={**self.cfg_data},\n",
    "            )\n",
    "            self.run_experiment()\n",
    "        \n",
    "    def run_experiment(self):\n",
    "        cfg_model = {\n",
    "            'n_hid_layers': 3,\n",
    "            'n_embeddings': self.cfg_data['n_embeddings'],\n",
    "            'emb_dim': self.tr_loader.dataset.embed_dim, # embedding dimensionality\n",
    "            'emb_latent_dim': 256, # embedding projection dimensionality\n",
    "            'transf_dim': self.tr_loader.dataset.tr_dim, # transformation dimensionality\n",
    "            'model_type': 'mlp', # 'attention', 'mlp'\n",
    "            'task': self.cfg_data['task'],\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 1e-3,\n",
    "            'use_layer_norm': 'batch',\n",
    "            'weight_decay': 1e-3,\n",
    "            'max_epochs': 100,\n",
    "            'num_classes': len(self.cfg_data['classes'])\n",
    "        }\n",
    "        wandb.config.update(cfg_model)\n",
    "        self._initialize_model(cfg_model)\n",
    "        self.train_model(cfg_model)\n",
    "        self.evaluate_model(self.tr_loader, 'train')\n",
    "        self.evaluate_model(self.va_loader, 'val')\n",
    "        wandb.finish()\n",
    "        \n",
    "    def run_sweep(self, sweep_id, project_name, group_name, run_name_prefix, count=10):\n",
    "        def sweep_function():\n",
    "            wandb.init(\n",
    "                project=project_name, \n",
    "                group=group_name,\n",
    "                config={**self.cfg_data},\n",
    "            )\n",
    "            run_name = f\"{run_name_prefix}_{wandb.run.id}\" # update sweep run name\n",
    "            wandb.run.name = run_name   \n",
    "            self.run_sweep_experiment()\n",
    "            \n",
    "        self.tr_loader, self.va_loader = get_data_loaders_wandb(self.cfg_data)\n",
    "        wandb.agent(sweep_id, sweep_function, count=count)\n",
    "\n",
    "    def run_sweep_experiment(self):\n",
    "        # Build model configuration from wandb.config\n",
    "        cfg_model = {\n",
    "            'n_hid_layers': wandb.config.n_hid_layers,\n",
    "            'n_embeddings': self.cfg_data['n_embeddings'],\n",
    "            'emb_dim': self.tr_loader.dataset.embed_dim,\n",
    "            'emb_latent_dim': wandb.config.emb_latent_dim,\n",
    "            'transf_dim': self.tr_loader.dataset.tr_dim,\n",
    "            'model_type': wandb.config.model_type,\n",
    "            'task': self.cfg_data['task'],\n",
    "            'dropout': wandb.config.dropout,\n",
    "            'learning_rate': wandb.config.learning_rate,\n",
    "            'use_layer_norm': wandb.config.use_layer_norm,\n",
    "            'weight_decay': wandb.config.weight_decay,\n",
    "            'max_epochs': wandb.config.max_epochs,\n",
    "            'num_classes': len(self.cfg_data['classes'])\n",
    "        }\n",
    "        self._initialize_model(cfg_model)\n",
    "        self.train_model(cfg_model)\n",
    "        self.evaluate_model(self.tr_loader, 'train')\n",
    "        self.evaluate_model(self.va_loader, 'val')\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_data = {\n",
    "    'n_workers': 32,\n",
    "    'batch_size': 2048,    \n",
    "    'stratification': 'majority', # 'random', 'binary', 'clustered', 'majority'\n",
    "    'data_path': '/mnt/fastdata/datasets/lore-loops/web-clang',\n",
    "    'embedding_model': 'source_coderankembed', # 'llvm_llmcompiler', 'source_codellama', 'source_llmcompiler', 'source_codet5p', 'source_coderankembed'\n",
    "    'embedding_layer': 'last', # 'last', 'all' (not working yet)\n",
    "    'n_embeddings': [1, 0, 1], # binary flags for [ref_embedding, transformation_embedding, transformation_encoding]\n",
    "    'min_transformations': 1, # only loop groups with at least <min> transformations\n",
    "    'max_transformations': 10000, # only loop groups with at most <max> transformations\n",
    "    'max_speedup': 2.5, # remove any loop group with a speedup transformation > max_speedup\n",
    "    'max_source_size': 100000, # only loop groups where reference .C file is at most <max> bytes\n",
    "    'task': 'classification',  # 'classification', 'regression'\n",
    "    # [(0, np.float64(0.5)), (2, np.float64(1.5)), (4, np.float64(2.5)), (6, np.float64(3.5))]\n",
    "    'class_splits': [0, .7, 1, 1.5, 17],\n",
    "    # 'class_splits': [-100, .5, 0.9999, 1.0001, 1.5, 2.5, 3.5, 100000],\n",
    "    'classes': ['HSL', 'SL', 'SP', 'HSP'],\n",
    "    'seed': 4,\n",
    "    'csv_path': './clang_4.0.0_data_table.csv', # for new data loader\n",
    "    'benchmark': '',\n",
    "    'filters': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815996db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9333a43",
   "metadata": {},
   "source": [
    "### Hyperparameter Sweep Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b92e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_data.update({\n",
    "    'benchmark': 'NPB',\n",
    "    'filters': [('benchmark', '==', 'NPB'), ('application', '==', 'SP')]\n",
    "})\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',  # 'random', 'grid', 'bayes'\n",
    "    'metric': {'name': 'validation_loss', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'n_hid_layers': {'values': [1, 2, 3, 4]},\n",
    "        'emb_latent_dim': {'values': [128, 256, 512]},\n",
    "        'model_type': {'value': 'mlp'},\n",
    "        'dropout': {'min': 0.1, 'max': 0.5},\n",
    "        'learning_rate': {'min': 1e-5, 'max': 1e-2},\n",
    "        'use_layer_norm': {'values': ['batch', 'layer']}, # or None but did bad\n",
    "        'weight_decay': {'min': 1e-5, 'max': 1e-3},\n",
    "        'max_epochs': {'value': 100},\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='MLFinal')\n",
    "\n",
    "runner = ExperimentRunner(cfg_data, device)\n",
    "runner.run_sweep(\n",
    "    sweep_id, \n",
    "    project_name='MLFinal', \n",
    "    group_name='NPB SP app, Experiment 6 (Random Sweep)', \n",
    "    run_name_prefix='exp6_run', \n",
    "    count=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c8864",
   "metadata": {},
   "source": [
    "### LOOCV Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_data.update({\n",
    "    'benchmark': 'NPB',\n",
    "    'filters': [('benchmark', '==', 'NPB')]\n",
    "})\n",
    "\n",
    "runner = ExperimentRunner(cfg_data, device, load_data=False)\n",
    "runner.run_loocv(\n",
    "    project_name='MLFinal', \n",
    "    group_name='LOOCV, NPB Benchmark', \n",
    "    run_name_prefix='exp2e', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254d1a3",
   "metadata": {},
   "source": [
    "### Standard Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60039579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_data.update({\n",
    "#     'benchmark': 'NPB',\n",
    "#     'filters': [('benchmark', '==', 'NPB'), ('application', '==', 'SP')]\n",
    "# })\n",
    "\n",
    "runner = ExperimentRunner(cfg_data, device)\n",
    "runner.run(\n",
    "    project_name='MLFinal', \n",
    "    group_name='All, Experiment 5d', \n",
    "    run_name_prefix='exp5d_clustered_strat_run',\n",
    "    count=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49a616",
   "metadata": {},
   "source": [
    "## Analysis of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a67593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg_data['csv_path'])\n",
    "\n",
    "# Print out how many data points and loop groups we have\n",
    "print(f'Total data points: {len(df)}')\n",
    "print(f'Number of loop groups: {len(df.groupby(\"id\"))}')\n",
    "print()\n",
    "\n",
    "# Print out the number of data points per class\n",
    "# class splits: [0, .7, 1, 1.5, 17]\n",
    "for i in range(len(cfg_data['class_splits']) - 1):\n",
    "    lower = cfg_data['class_splits'][i]\n",
    "    upper = cfg_data['class_splits'][i + 1]\n",
    "    num_points = len(df[(df['speedup_r'] >= lower) & (df['speedup_r'] < upper)])\n",
    "    print(f'Number of data points with speedup between {lower} and {upper}: {num_points}, {num_points / len(df) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of data points per loop group\n",
    "num_points_per_group = df.groupby('id').size()\n",
    "print(f'Max number of data points per loop group: {num_points_per_group.max()}')\n",
    "print(f'Min number of data points per loop group: {num_points_per_group.min()}')\n",
    "print(f'Median number of data points per loop group: {num_points_per_group.median()}')\n",
    "print(f'Mean number of data points per loop group: {num_points_per_group.mean()}')\n",
    "print(f\"Mode number of data points per loop group: {num_points_per_group.mode()[0]}\")\n",
    "print(f'Standard deviation of the number of data points per loop group: {num_points_per_group.std()}')\n",
    "\n",
    "# Make a scatter plot of the number of data points per loop group\n",
    "plt.scatter(range(len(num_points_per_group)), num_points_per_group)\n",
    "plt.xlabel('Loop Group ID')\n",
    "plt.ylabel('Number of Data Points')\n",
    "plt.title('Number of Data Points per Loop Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of how many data points are available for each benchmark\n",
    "fig, ax = plt.subplots()\n",
    "df['benchmark'].value_counts().plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Benchmark')\n",
    "ax.set_ylabel('Number of Data Points')\n",
    "ax.set_title('Number of Data Points per Benchmark')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add numbers at the top of each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2, height, f'{height}', ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of how many loop groups are available for each benchmark\n",
    "fig, ax = plt.subplots()\n",
    "df.groupby('benchmark')['id'].nunique().reindex(df['benchmark'].value_counts().index).plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Benchmark')\n",
    "ax.set_ylabel('Number of Loop Groups')\n",
    "ax.set_title('Number of Loop Groups per Benchmark')\n",
    "\n",
    "# Add numbers at the top of each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2, height, f'{height}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the unique applications in the 'NPB' benchmark\n",
    "npb_apps = df[df['benchmark'] == 'NPB']['application'].unique()\n",
    "print(f'Unique applications in the NPB benchmark: {npb_apps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec691c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of how many data points are available for each NPB benchmark application\n",
    "fig, ax = plt.subplots()\n",
    "bar_plot = df[df['benchmark'] == 'NPB']['application'].value_counts().sort_index().plot(kind='bar', ax=ax, color='purple')\n",
    "ax.set_xlabel('Application')\n",
    "ax.set_ylabel('Number of Data Points')\n",
    "ax.set_title('Number of Data Points per NPB Benchmark Application')\n",
    "ax.set_yscale('log')  # Set y-axis to logarithmic scale\n",
    "\n",
    "# Add numbers at the top of each bar\n",
    "for bar in bar_plot.patches:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of how many loop groups are available for each NPB benchmark application\n",
    "fig, ax = plt.subplots()\n",
    "bar_plot = df[df['benchmark'] == 'NPB'].groupby('application')['id'].nunique().sort_index().plot(kind='bar', ax=ax, color='green')\n",
    "ax.set_xlabel('Application')\n",
    "ax.set_ylabel('Number of Loop Groups')\n",
    "ax.set_title('Number of Loop Groups per NPB Benchmark Application')\n",
    "\n",
    "# Add numbers at the top of each bar\n",
    "for bar in bar_plot.patches:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, height, f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how much of the total loop groups and total data points are in the NPB benchmark\n",
    "npb_df = df[df['benchmark'] == 'NPB']\n",
    "total_loop_groups = len(df['id'].unique())\n",
    "total_data_points = len(df)\n",
    "npb_loop_groups = len(npb_df['id'].unique())\n",
    "npb_data_points = len(npb_df)\n",
    "print(f'{npb_loop_groups} / {total_loop_groups} loop groups are in the NPB benchmark ({npb_loop_groups / total_loop_groups * 100:.2f}%)')\n",
    "print(f'{npb_data_points} / {total_data_points} data points are in the NPB benchmark ({npb_data_points / total_data_points * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5384013",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# REQUIRES A SECOND LOOK\n",
    "##########################################\n",
    "class BidirectionalCrossAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.n_embeddings = cfg['n_embeddings']\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], cfg['emb_latent_dim']),\n",
    "            nn.LayerNorm(cfg['emb_latent_dim'])\n",
    "        )\n",
    "        latent_dim = cfg['emb_latent_dim'] * self.n_embeddings\n",
    "        out_dim = 1 if cfg['task'] == 'regression' else cfg['num_classes']\n",
    "        self.head = MLPHead(latent_dim+cfg['transf_dim'], out_dim, cfg['use_layer_norm'], cfg['dropout'], cfg['n_hid_layers'])\n",
    "        self.cross_attention1 = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=4, dropout=cfg['dropout'], batch_first=True, kdim=cfg['transf_dim'], vdim=cfg['transf_dim'])\n",
    "        self.cross_attention2 = nn.MultiheadAttention(embed_dim=cfg['transf_dim'], num_heads=4, dropout=cfg['dropout'], batch_first=True, kdim=latent_dim, vdim=latent_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        emb_proj_1 = self.projection(batch[0])\n",
    "        if self.n_embeddings == 1:\n",
    "            features1, _ = self.cross_attention1(query=emb_proj_1, key=batch[1], value=batch[1])\n",
    "            features2, _ = self.cross_attention2(query=batch[1], key=emb_proj_1, value=emb_proj_1)\n",
    "            return self.head(torch.cat([emb_proj_1+features1, batch[1]+features2], dim=1)).squeeze(-1)\n",
    "        elif self.n_embeddings == 2:\n",
    "            emb_proj_2 = self.projection(batch[1])\n",
    "            embs = torch.cat([emb_proj_1, emb_proj_2], dim=1)\n",
    "            features1, _ = self.cross_attention1(query=embs, key=batch[2], value=batch[2])\n",
    "            features2, _ = self.cross_attention2(query=batch[2], key=embs, value=embs)\n",
    "            return self.head(torch.cat([embs+features1, batch[2]+features2], dim=1)).squeeze(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
